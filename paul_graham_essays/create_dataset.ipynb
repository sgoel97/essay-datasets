{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Paul Graham Essays Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process PG Essays txt\n",
    "\n",
    "Raw txt can be found here: https://github.com/dbredvick/paul-graham-to-kindle/blob/main/paul_graham_essays.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"./text_data\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"./dataset\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/dbredvick/paul-graham-to-kindle/main/paul_graham_essays.txt\"\n",
    "response = requests.get(url)\n",
    "with open(\"./text_data/paul_graham_essays.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load txt file with all essays\n",
    "\n",
    "pg_essay_path = Path(\"./text_data/paul_graham_essays.txt\")\n",
    "pg_essay_text = pg_essay_path.read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate Each Essay\n",
    "\n",
    "essays = pg_essay_text.split(\"\\n# \")[2:-1]\n",
    "essays = list(map(lambda s: \"# \" + s, essays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "filtered_strings = [\n",
    "    \"Translation](\",\n",
    "    \"**Want to start a startup?** Get funded by [Y Combinator](http://ycombinator.com/apply.html).\",\n",
    "    \"[](https://sep.yimg.com\",\n",
    "    \"**Like to build things?** Try [Hacker News](http://news.ycombinator.com).\",\n",
    "    \"Watch how this essay was [written](https://code.stypi.com/hacks/13sentences?doomed=true).\",\n",
    "    \"[Comment](http://news.ycombinator.com\",\n",
    "    \"[](http://reddit.com)[Comment](http://reddit.com\",\n",
    "    \"the mere consciousness of an engagement will sometimes worry a whole day\",\n",
    "    \"ï¿½ Charles Dickens\",\n",
    "]\n",
    "\n",
    "for i in range(len(essays)):\n",
    "    essays[i] = essays[i].replace(\"* * *\", \"\")\n",
    "    essays[i] = essays[i].replace(\"[](index.html)\", \"\")\n",
    "    essays[i] = essays[i].strip()\n",
    "    essays[i] = re.sub(r\"(\\s*\\n\\s*){2,}\", \"\\n\\n\", essays[i])\n",
    "    essays[i] = \"\\n\\n\".join(\n",
    "        list(\n",
    "            filter(\n",
    "                lambda s: all([f not in s for f in filtered_strings]),\n",
    "                essays[i].split(\"\\n\\n\"),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    essays[i] = re.sub(r\"(\\s*\\n\\s*){2,}\", \"\\n\\n\", essays[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each essay\n",
    "\n",
    "for i, essay in enumerate(essays):\n",
    "    output_path = Path(f\"./text_data/{i}.txt\")\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    output_path.write_text(essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the original txt file after processing\n",
    "\n",
    "os.remove(\"./text_data/paul_graham_essays.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Raw Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each text file, adding them to the dataset\n",
    "\n",
    "dataset = []\n",
    "collection_path = Path(\"./text_data\")\n",
    "txt_files = collection_path.glob(\"*.txt\")\n",
    "\n",
    "collection_data = []\n",
    "for file in txt_files:\n",
    "    text = file.read_text()\n",
    "\n",
    "    # Change dataset columns here\n",
    "    file_data = {\n",
    "        \"text\": text,\n",
    "    }\n",
    "\n",
    "    dataset.append(file_data)\n",
    "\n",
    "# Convert data to pandas DataFrame\n",
    "df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date helpers and constants\n",
    "\n",
    "months = [\n",
    "    \"January\",\n",
    "    \"February\",\n",
    "    \"March\",\n",
    "    \"April\",\n",
    "    \"May\",\n",
    "    \"June\",\n",
    "    \"July\",\n",
    "    \"August\",\n",
    "    \"September\",\n",
    "    \"October\",\n",
    "    \"November\",\n",
    "    \"December\",\n",
    "]\n",
    "years = list(map(str, range(1900, 2032)))\n",
    "dates = list(map(lambda x: x[0] + \" \" + x[1], itertools.product(months, years))) + years\n",
    "\n",
    "# Assign a unique index to each date capturing their relative ordering\n",
    "dates_indices = {}\n",
    "for month, year in itertools.product(months, years):\n",
    "    dates_indices[month + \" \" + year] = int(year) * 15 + months.index(month) + 1\n",
    "    dates_indices[year] = int(year) * 15\n",
    "\n",
    "\n",
    "# Returns the latest (month, year) pair from a list of date strings\n",
    "def latest_date(dates):\n",
    "    return max(dates, key=lambda d: dates_indices.get(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each essay to extract the title and date while cleaning remaining text\n",
    "\n",
    "for i in range(len(df)):\n",
    "    df.iloc[i, 0] = re.sub(r\"(\\s*\\n\\s*){2,}\", \"\\n\\n\", df.iloc[i, 0]).strip()\n",
    "    sentences = df.iloc[i, 0].split(\"\\n\\n\")\n",
    "\n",
    "    # Extract and remove date\n",
    "    if sentences[1] in dates:\n",
    "        df.loc[i, \"date\"] = sentences[1]\n",
    "        sentences.pop(1)\n",
    "    elif sentences[2] in dates:\n",
    "        df.loc[i, \"date\"] = sentences[2]\n",
    "        sentences.pop(2)\n",
    "    elif (\"rev\" in sentences[1] or \"corrected\" in sentences[1]) and sum(\n",
    "        [d in sentences[1] for d in dates]\n",
    "    ) > 1:\n",
    "        all_dates = [d for d in dates if d in sentences[1]]\n",
    "        df.loc[i, \"date\"] = latest_date(all_dates)\n",
    "        sentences.pop(1)\n",
    "    else:\n",
    "        df.loc[i, \"date\"] = None\n",
    "\n",
    "    # Rejoin remaining sentences, excluding title\n",
    "    df.iloc[i, 0] = \"\\n\\n\".join(sentences[1:])\n",
    "\n",
    "    # Extract and remove title\n",
    "    df.loc[i, \"title\"] = sentences[0].replace(\"# \", \"\")\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    df.iloc[i, 0] = re.sub(r\"(\\s*\\n\\s*){2,}\", \"\\n\\n\", df.iloc[i, 0]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle date edge case\n",
    "\n",
    "df.loc[df[\"date\"] == \"1993\", \"date\"] = \"September 1993\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename and reorder columns\n",
    "\n",
    "df.index.name = \"id\"\n",
    "df = df[[\"title\", \"date\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "\n",
    "df = df.drop_duplicates(subset=[\"text\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "\n",
    "df.to_csv(\"./dataset/pual_graham_essays.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
